# Homework #3: Optimization

## Matrix Library

&nbsp; The first part of HW3 required creation of a Matrix and Neural Network Library. <br>
&nbsp; Contained in this project are the following files:

- ```network-solver.cpp```
    - ```main```
        - Put networks to make in a text file ```networks.txt```
        - Ensure structure of ```networks.txt``` is exactly the same as the provided ```networks.txt```
            - including the example input and output lines
        - Ensure file is at same level as ```network-solver.cpp```
        - Running program will construct each network and attempt to optimize using Cross Entropy
        - Outputs will be sent to a text file ```netout.txt```
        - Outputs include the minimizing vector and the magnitude of the output
        - If you wish to test an input:
        - Create text file ```solutions.txt``` at same level as ```networks.txt```
        - Put a single input vector on each line corresponding to the networks in ```networks.txt```
            - i.e. input vector for network 1 should go on line 1, input 2 on line 2, etc...
        - Change the conditions of a couple ifs in main (I was too lazy to make this better)
        - Run program and the output and magnitude of output will be put to stdout
- ```Network.h / Network.cpp```
    - Contains logic for the network structure
    - ```Network(int _numLayers, Layer** _layers)```
        - Takes in the number of layers and a preconstructed array of Layer pointers
        - Assigns class variables
    - ```~Network()```
        - Destructor
    - ```Random(int maxIter, bool inf)```
        - Initiates random inputs
        - Solves for the output of the network
        - Compares output to best and saves best if less
        - Iterates either until ```best < 0.000001``` or, if ```inf == false```, ```currIter == maxIter```
        - Once the loop exits, information about best is printed, and the best input is returned
    - ```RandomSearch(int maxIter, bool inf)```
        - Similar to Random
        - Gets a random first input
        - Each iteration, randomly adds a small amount to the input indices
        - Predicts based off new input
        - Keeps new input if it is better
        - Prints info once converged and returns best input
    - ```Gradient(int maxIter, bool inf)```
        - Not implemented
    - ```GradientMomentum(int maxIter, float beta, bool inf)```
        - Not implemented
    - ```GradientMomentumScale(int maxIter, float beta, float alpha, bool inf)```
        - Not implemented
    - ```CrossEntropy(int maxIter, bool inf)```
        - Initiates a number of samples to generate and a number of samples to keep
        - Initiates vectors of means and standard deviations for each element of the input
        - Initiates N inputs, each element of an input being generated by a gaussian distribution
        - Each index of the input vector has its own mean and standard deviation
        - Gets the magnitude of each sample's output
        - Sorts the samples from lowest magnitude to highest
        - From the top Ne samples, the new mean and standard deviation for each index is found
        - The standard deviation then has some noise added to it based on iteration number
        - Once converged, the mean vector is considered the best input and it is returned
    - ```Predict(Matrix& input)```
        - Takes in input vector
        - Applies the formula for each layer in the network
        - If relu: ```relu(Wx + b)```
        - If not relu: ```Wx + b```
        - Can also handle sigmoid activation function (with some modifications to the main method)
        - Returns output of the network
    - ```toString()``` 
        - outputs a comprehensive representation of the network structure 
        - includes weights, biases, activation functions, and dimensions
    - getters / setters
- ```Layer.h / Layer.cpp```
    - Contains logic for each individual layer of the network
    - ```Layer(int _rows, int _cols, bool _relu, bool _sigmoid, float* _weights, float* _biases)```
        - Constructor one that sets up the network 
        - Takes in float**'s for weights and biases casted to float\*
    - ```Layer(int _rows, int _cols, bool _relu, bool _sigmoid, Matrix* _weights, Matrix* _biases)```
        - Constructor two that sets up the network
        - Takes in Matrix*'s for weights and biases
    - ```~Layer()```
        - Destructor
    - ```toString()```
        - returns a string that represents the current layer
    - getters / setters
- ```Matrix.h / Matrix.cpp```
    - Contains logic for matrices, their creation, and some of their manipulation
    - ```Matrix()```
        - Default constructor
        - Don't use this
    - ```Matrix(const Matrix& o)```
        - Copy constructor
        - copies an already created matrix into this one
    - ```Matrix(int rows, int cols)```
        - Creates a matrix of all 0's with provided dimensions
    - ```Matrix(float* mat, int _rows, int _cols)```
        - Creates a matrix from a float** casted to float*
    - ```~Matrix()```
        - Destructor
    - overloaded operators
        - Multiplication: both * and *=
            - Multiplying a matrix by another matrix with one column and equal rows will multiply each row by the corresponding row of the second
            - Multiplying a matrix by a scalar will multiply each element of the matrix by the scalar
            - Multiplying a matrix by another with same dimensions will multiply each element by the corresponding element
            - Other combinations will result in an error message and an EXIT_FAILURE
            - This should not be used for matrix multiplication
        - Addition: both + and += 
            - Same rules apply as above for the most part
        - Assignment: =
            - Sets the current matrix equal to the values of the other matrix
            - Still two seperate objects
        - []
            - Used for indexing the matrix
            - if Matrix is a pointer, must do (*mat)[]
            - [] takes in a ```loc``` struct
            - ```loc``` structs can be constructed as such: ```loc(x,y)```
                - x is row, y is column
            - Indexing would look as such for accessing the top left most element
                - ```mat[loc(0,0)]``` or ```(*mat)[loc(0,0)]``` if pointer
    - getters / setters
    - ```toString()```
        - returns string representation of matrix
- ```utils.h```
    - Contains helpful functions for proper functionality of the system
    - ```eraseFromString(string str, char delim)```
        - Erases all instances of the character ```delim``` from copy of string ```str```
        - Returns the edited copy
    - ```split(string str, string delim)```
        - Splits the string ```str``` into tokens separated by the string ```delim```
        - Does not modify string passed through (I think)
        - Returns vector of string tokens
    - ```vectorMean(Matrix* vec)```
        - I don't know why I made this
        - I don't think its used anywhere
    - ```PairCompare(pair<Matrix*, float> p1, pair<Matrix*, float> p2)```
        - Used for the sorting of the sample vector in Cross Entropy
    - ```magnitude(Matrix* mat)```
        - Finds the absolute value sum of the passed in matrix
    - ```arbitraryRand(float low, float high)```
        - Gets a random number between ```low``` and ```high```
    - ```dot(Matrix& m1, Matrix& m2)```
        - Didn't actually implement this one lol
        - Just returns 0
        - Don't use
    - ```matMul(Matrix& m1, Matrix& m2)```
        - Performs matrix multiplication on two matrices
        - Matrices must be of compatible dimensions otherwise an error message is shown and the program exits
    - ```sigmoid(float val)```
        - Performs the sigmoid function on a value
    - ```relu(float val)```
        - Performs the relu function on a value

## Optimization Methods

&nbsp; The optimization methods actually implemented in this library are Cross Entropy, Random Search, and just plain Random. <br>
&nbsp; The functionality of each specific method is described above in the corresponding function description. <br>
&nbsp; Random Search and Random were very straightforward so I don't think they require much analysis. <br>
&nbsp; Random yielded good results if you let it run enough and Random Search's results were highly dependent on the intial input. <br>
&nbsp; Cross Entropy was where things got interesting, and I got it working after a bit of consultation with Prof. Guy. <br>
&nbsp; Initially, it was giving me good results, but not quite as good as they could be. At that point, I was using roughly 1000 samples and keeping 200. <br>
&nbsp; After playing around with a couple sample numbers, I changed the initial standard deviation to less than 1.0. <br>
&nbsp; This yielded slightly better results for some of the networks that CEM was struggling on. <br>
&nbsp; The final piece of the puzzle that I was missing however, was the number of samples that I was keeping. <br>
&nbsp; It turns out that having a very low number of samples kept per each iteration yields much better results for most of the networks. <br>
&nbsp; This obviously might not be the case for many situations, but it is what worked here, and, in the end, that is what ML is about. 


## Credits

- Professor Guy for understanding CEM and some tricks
- cppreference.com for A LOT
- My brain for everything else